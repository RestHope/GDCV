{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "comparative-british",
   "metadata": {},
   "source": [
    "## 1. 백본 네트워크 구조 상세분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-enlargement",
   "metadata": {},
   "source": [
    "### 학습목표\n",
    "\n",
    "- 딥러닝 논문의 구조에 익숙해지기\n",
    "- 네트워크를 구조화하는 여러 방법의 발전 양상을 알아보기\n",
    "- 새로운 논문 찾아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controversial-voltage",
   "metadata": {},
   "source": [
    "### 학습내용\n",
    "\n",
    "- 딥러닝 논문의 구조\n",
    "- ResNet의 핵심 개념과 그 효과\n",
    "- ResNet 이후 시도 (1) Connection을 촘촘히\n",
    "- ResNet 이후 시도 (2) 어떤 특성이 중요할까?\n",
    "- 모델 최적화하기 (1) Neural Architecture Search\n",
    "- 모델 최적화하기 (2) EfficientNet\n",
    "- 직접 찾아보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-fishing",
   "metadata": {},
   "source": [
    "### 1. 딥러닝 논문의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-collaboration",
   "metadata": {},
   "source": [
    "- 원본논문 : https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-cargo",
   "metadata": {},
   "source": [
    "#### 1-1. 논문요약\n",
    "\n",
    "- Degradation 문제를 해결하기 위해 residual connection(ResNet)을 적용했다.\n",
    "        \n",
    "        네트워크 층을 더 쌓으면서 깊은 네트워크를 구현하여 성능향상을 이루고 있는데\n",
    "        실제로 어느정도 이상 깊어진 네트워크는 \n",
    "        오히려 vanishing/exploding gradient(기울기소실/폭주)문제 때문에 성능이 더 떨어진다.\n",
    "        \n",
    "        \n",
    "- 네트워크가 깊어짐에 따라 증가하는 복잡도를 줄이기 위해 Bottleneck을 적용했다.\n",
    "- 해당 시기의 Image classification, Object detection, \n",
    "<br/> Semantic Segmentation 등의 task에서 좋은 성능을 보여줬다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-spyware",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. ResNet의 핵심개념과 그 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-shelf",
   "metadata": {},
   "source": [
    "#### 2-1. RestNet 논문의 문제의식\n",
    "    레이어를 깊이 쌓았을 때 Vanishing/Exploding Gradient 문제가 발생하여 \n",
    "    모델의 수렴을 방해하는 문제가 생기는데, \n",
    "    여기에 대해서는 이미 몇 가지 대응 방법이 알려져 있다.\n",
    "    \n",
    "    레이어를 깊이 쌓았을 때 모델이 수렴하고 있음에도 불구하고 발생하는 문제.\n",
    "    해결책으로는 normalized initialization, intermediate normalization layers\n",
    "    \n",
    "    Degradation Problem ?\n",
    "    딥러닝 모델의 레이어가 깊어졌을 때 모델이 수렴했음에도 불구하고 \n",
    "    오히려 레이어 개수가 적을 때보다 모델의 training/test error가 \n",
    "    더 커지는 현상이 발생하는데, 이것은 오버피팅 때문이 아니라 \n",
    "    네트워크 구조상 레이어를 깊이 쌓았을 때 최적화가 잘 안되기 때문에 발생하는 문제."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-possession",
   "metadata": {},
   "source": [
    "#### 2-2. ResNet 논문이 제시한 솔루션 : Residual Block\n",
    "    일종의 지름길(\"shortcut connection\")을 통해서 \n",
    "    레이어가 입력값을 직접 참조하도록 레이어를 변경했다고 봄.\n",
    "    \n",
    "    Shortcut connection ?\n",
    "    앞에서 입력으로 들어온 값을 네트워크의 출력층에 곧바로 더해줌.\n",
    "    ResNet이라는 이름을 가진 이유는 출력값에서 원본 입력을 제외한 잔차(residual) 함수를 학습하기 때문\n",
    "    \n",
    "    H(x)를 F(x)+xF(x)+x로 만들면 어떨까? 여기서 x는 레이어의 입력값입니다.\n",
    "    설령 F(x)F(x)가 Vanishing Gradient현상으로 전혀 학습이 안되어 zero mapping이 될지라도, \n",
    "    최종 H(x)H(x)는 최소한 identity mapping이라도 될 테니 성능 저하는 발생하지 않게 된다는 것.\n",
    "    \n",
    "    실험해 보니 이 구조가 실제로도 안정적으로 학습이 되며, \n",
    "    레이어를 깊이 쌓을수록 성능이 향상되는 것이 확인되었기 때문.\n",
    "    \n",
    "    ResNet에서는 shortcut connection을 가진 ResNet의 기본 블록을 Residual Block이라고 부른다.\n",
    "    ResNet은 이러한 Residual Block 여러 개로 이루어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-ability",
   "metadata": {},
   "source": [
    "#### 2-3. Experiments\n",
    "    shortcut connection이 없는 네트워크와 이를 사용한 네트워크를 가지고 성능을 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-riverside",
   "metadata": {},
   "source": [
    "---\n",
    "### 3-1. RestNet 이후 시도 <br/> - Connection을 촘촘히"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-webster",
   "metadata": {},
   "source": [
    "- 원본 논문 : https://arxiv.org/abs/1608.06993\n",
    "- Pre-activation 논문 : https://arxiv.org/pdf/1603.05027.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-guide",
   "metadata": {},
   "source": [
    "#### 3-1. Dense Connectivity\n",
    "    경사 소실 문제(gradient vanishing)를 개선하고 특성을 계속 재사용할 수 있도록 함.\n",
    "    \n",
    "    ResNet은 shortcut을 원소별로 단순히 더해주었던 반면, \n",
    "    DenseNet은 하나하나를 차원으로 쌓아서(concatenate) 하나의 텐서로 만들어 낸다는 사실이 차이점이다.\n",
    "    또한 배치 정규화(BN), ReLU 활성화 함수, 그리고 3x3 컨볼루션 레이어를 통해서 pre-activation을 수행.\n",
    "    \n",
    "    pre-activation ?\n",
    "    ReLU가 컨볼루션 블록 안으로 들어간 것을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-donor",
   "metadata": {},
   "source": [
    "#### 3-2. Growth Rate\n",
    "    DenseNet에서는 특성 맵을 채널 방향으로 쌓아서 사용.\n",
    "    입력값의 채널이 4로 시작했으나 진행할수록 특성 맵의 크기가 매우 커지는 것을 볼 수 있다.\n",
    "    블록 내의 채널 개수를 작게 가져가면서 최종 출력값의 특성 맵 크기를 조정할 수 있도록 했다고 함.\n",
    "    \n",
    "    bottleneck 레이어, transition 레이어, composite function의 여러 방식도 적용."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjusted-trainer",
   "metadata": {},
   "source": [
    "---\n",
    "### 3-2. RestNet 이후 시도 <br/> - 어떤 특성이 중요할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-abuse",
   "metadata": {},
   "source": [
    "- 원본 논문 : https://arxiv.org/abs/1709.01507"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-roman",
   "metadata": {},
   "source": [
    "#### 3-1. Squeeze\n",
    "    특성에서 중요한 정보를 짜내는 과정.\n",
    "    풀링(pooling) 기법을 사용하면 됩니다. 풀링은 보통 커널(kernel) 영역의 정보를 압축하는 데 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-determination",
   "metadata": {},
   "source": [
    "#### 3-2. Excitate\n",
    "    채널별 정보에 따라 어떤 채널을 강조해야 할지 판단.\n",
    "    시그모이드를 사용하는데 그 이유가 가장 중요한 하나의 채널만 활성화되는 것이 아닌, \n",
    "    여러 채널들이 서로 다른 정도로 활성화되도록 하기 위함.\n",
    "    \n",
    "    데이터셋에 정답 라벨이 하나뿐인 단순 분류 모델의 활성화 함수로는 \n",
    "    소프트맥스(SoftMax)를 사용해서 단 하나의 최댓값을 찾지만, 하나의 대상에도 \n",
    "    여러 개의 클래스의 정답 라벨을 지정할 수 있는 \n",
    "    다중 라벨 분류(multi label classification)에서는 시그모이드를 사용하는 것과 같은 방식입니다.\n",
    "    \n",
    "    계산된 벡터를 기존의 특성 맵에 채널에 따라서 곱해주어 중요한 채널이 활성화 되도록 만들어준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-rings",
   "metadata": {},
   "source": [
    "---\n",
    "### 4-1. 모델 최적화하기 <Br/> - Neural Architecture Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-administrator",
   "metadata": {},
   "source": [
    "- 원본 논문 : https://arxiv.org/abs/1707.07012"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-research",
   "metadata": {},
   "source": [
    "#### 4.1 NASNet\n",
    "    일반적으로 머신 러닝에서는 그리드 탐색(grid search) 등으로 실험과 \n",
    "    모델 셋팅(config)를 비교하기 위한 자동화된 방법을 사용한다.\n",
    "    \n",
    "    NASNet 논문은 이미지넷 데이터에 대해 이보다 짧은 시간 안에 최적화를 했다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-symbol",
   "metadata": {},
   "source": [
    "#### 4.2 Convolution cell\n",
    "    탐색 공간을 줄이기 위해서 모듈(cell) 단위의 최적화를 하고 그 모듈을 조합하는 방식을 채택한다.\n",
    "    Residual Block, DenseNet에는 Dense Block이라는 모듈이 사용 → convolution cell\n",
    "    normal cell과 reduction cell로 구분된다.\n",
    "    \n",
    "    Normal cell은 특성 맵의 가로, 세로가 유지되도록 stride를 1로 고정합니다. \n",
    "    Reduction cell은 stride를 1 또는 2로 가져가서 특성 맵의 크기가 줄어들 수 있도록 합니다.\n",
    "    \n",
    "    두 가지 cell을 조합해 것이 최종 결과 네트워크(NASNet)를 만들었으며, \n",
    "    좀 더 적은 연산과 가중치로 SOTA(state-of-the-art) 성능을 기록했음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-software",
   "metadata": {},
   "source": [
    "---\n",
    "### 4-2. 모델 최적화하기 <Br/> - EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-scott",
   "metadata": {},
   "source": [
    "- 원본 논문 ; https://arxiv.org/abs/1905.11946"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-money",
   "metadata": {},
   "source": [
    "#### 4.1 EfficientNet\n",
    "    기존 모델들의 오류율을 뛰어넘을 뿐만 아니라 \n",
    "    모델의 크기인 \"Number of Parameters\" 또한 최적화된 것을 볼 수 있다.\n",
    "    \n",
    "    CNN을 효율적으로 사용할 수 있도록 \n",
    "    네트워크의 형태를 조정할 수 있는 width, depth, resolution 세 가지 요소에 집중한다.\n",
    "    \n",
    "    width는 CNN의 채널.\n",
    "    채널을 늘려줄수록 CNN의 파라미터와 특성을 표현하는 차원의 크기를 키울 수 있다.\n",
    "    \n",
    "    depth는 네트워크의 깊이. \n",
    "    ResNet은 대표적으로 네트워크를 더 깊게 만들 수 있도록 설계해 성능을 올린 예시이다.\n",
    "    \n",
    "    resolution은 입력값의 너비(w)와 높이(h) 크기.\n",
    "    입력이 클수록 정보가 많아져 성능이 올라갈 여지가 생기지만 레이어 사이의 특성 맵이 커지는 단점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-upset",
   "metadata": {},
   "source": [
    "#### 4.2 Compound scaling\n",
    "    resolution, depth, width를 최적으로 조정하기 위해서 \n",
    "    앞선 NAS와 유사한 방법을 사용해 기본 모델(baseline network)의 구조를 미리 찾고 고정한다.\n",
    "    고정이 되면 효율적인 모델을 찾는다는 커다란 문제가, \n",
    "    개별 레이어의 resolution, depth, width 를 조절해 기본 모델을 적절히 확장시키는 문제로 단순화된다.\n",
    "    \n",
    "    논문에서는 scaling factor\"를 동시에 고려하는 compound scaling을 제안한다.\n",
    "    각각 조정하는 것이 아니라 고정된 계수에 변하도록 하면 \n",
    "    보다 일정한 규칙에 따라(in a principled way) 모델의 구조가 조절되도록 할 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
